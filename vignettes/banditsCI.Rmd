---
title: "banditsCI"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{my-vignette}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(banditsCI)
library(ggplot2)
set.seed(123)
cbPalette <- c("#999999", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", 
               "#D55E00", "#CC79A7")

```

Set parameters. Here, we generate a dataset with 1000 observations in 4 equally 
sized batches, 5 covariates, and 4 arms. 
```{r parameters}
floor_start <- 5 # TODO discuss
floor_decay <- 0.9 # TODO discuss
batch_sizes <- c(250, 250, 250, 250)
A <- sum(batch_sizes)
p <- 5
K <- 4
```

Simulate data from an interacted linear model. 
```{r simlindata}
# Interacted linear model
X <- matrix(rnorm(A*p), ncol=p) # - X: covariates of shape [A, p]
# generate a linear model
coef <- c(rnorm(2), rnorm(ncol(X)-2, sd = 0.05))
y_latent <- X %*% coef
y <- as.numeric(cut(rank(y_latent),
                    # one group is twice as large as other groups
                    breaks = c(0, A*(2:(K+1)/(K+1))) ))
data <- generate_bandit_data(X = X, y = y, noise = 0)

```

Alternatively, we could use tree data (this is not evaluated here). 
```{r simtreedata, eval = FALSE}
data <- simple_tree_data(
  A=A,
  K=K,
  p=p,
  split = 0.6,
  noise_std = 0.5)
```

## For the contextual case
We run a contextual experiment. 
```{r contextual}
# access dataset components
xs <- data[[1]]$xs
ys <- data[[1]]$ys
results <- run_experiment(ys, floor_start, floor_decay, batch_sizes, xs)

# conditional means model
# mu_hat <- calculate_mu_hat(results)
mu_hat <- ridge_muhat_lfo_pai(xs=xs, ws=results$ws, yobs=results$yobs, K=K, batch_sizes=batch_sizes)

# inverse probability score 1[W_t=w]/e_t(w) of pulling arms, shape [A, K]
balwts <- calculate_balwts(results$ws, results$probs)

# plot the cumulative assignment graph for every arm and every batch size, x-axis is the number of observations, y-axis is the cumulative assignment
plot_cumulative_assignment(results, batch_sizes)


aipw_scores_learn <- aw_scores(
  ws = results$ws, 
  yobs = results$yobs, 
  mu_hat = mu_hat[A,,], 
  K = ncol(results$ys),
  balwts = balwts)

```

```{r estimation}
# Get estimates for policies

## Define counterfactual policies
### Control policy matrix policy0. This is a matrix with A rows and K columns, where the elements in the first column are all 1s and the elements in the remaining columns are all 0s.
policy0 <- matrix(0, nrow = A, ncol = K)
policy0[,1] <- 1

### Treatment policies list policy1. This is a list with K elements, where each list contains a matrix with A rows and K columns. Identifier of treatment x: the x th column of the matrix in the x th policy in the list is 1.
policy1 <- lapply(1:K, function(x) {
  pol_mat <- matrix(0, nrow = A, ncol = K)
  pol_mat[,x] <- 1
  pol_mat
}
) 

## estimating the value Q(w) of a single arm w. Here we estimate all the arms in policy1 in turn. 
out_full <- output_estimates(
  policy1 = policy1, 
  gammahat = aipw_scores_learn, 
  contextual_probs = results$probs)

# Get estimates for treatment effects of policies as contrast to control \delta(w_1, w_2) = E[Y_t(w_1) - Y_t(w_2)]. In Hadad, Vitor, et al (2021) there are two approaches.
## The first approach: use the difference in AIPW scores as the unbiased scoring rule for \delta (w_1, w_2)
### The following function implements the first approach by subtracting policy0, the control arm, from all the arms in policy1, except for the control arm itself.
out_full_te1 <- output_estimates(
  policy0 = policy0,
  policy1 = policy1[-1],  ## remove the control arm from policy1
  contrasts = 'combined',
  gammahat = aipw_scores_learn, 
  contextual_probs = results$probs)

## The second approach takes asymptotically normal inference about \delta(w_1, w_2): \delta ^ hat (w_1, w_2) = Q ^ hat (w_1) - Q ^ hat (w_2)
out_full_te2 <- output_estimates(
  policy0 = policy0,
  policy1 = policy1[-1],  ## remove the control arm from policy1
  contrasts = 'separate',
  gammahat = aipw_scores_learn, 
  contextual_probs = results$probs)


# Figure

# Combine the data frames into a single data frame
combinedMatrix <- do.call(rbind, out_full_te2)
combinedMatrix <- as.data.frame(combinedMatrix)

# Add an .id column to indicate the arm number
n_arms <- length(out_full_te2)
n_reps <- nrow(out_full_te2[[1]])
combinedMatrix$id <- rep(1:n_arms, each = n_reps)

# Add a column to indicate the arm name
n_arms <- n_arms+1
combinedMatrix$arm <- rep(paste0("arm", 2:n_arms), each = n_reps)

# Add a method column
values <- c("uniform", "non_contextual_minvar", "contextual_minvar", 
            "non_contextual_stablevar", "contextual_stablevar", "non_contextual_twopoint")
combinedMatrix$method <- rep(values, length.out = nrow(combinedMatrix))


# Create the plot
plot <- ggplot(combinedMatrix, aes(x = estimate, y = method)) +
  geom_point(aes(x = estimate), size = 2, position = position_dodge(width=0.5)) +
  geom_errorbar(aes(xmin = estimate - 1.96*std.error, xmax = estimate + 1.96*std.error), width = 0.05, 
                position = position_dodge(width=0.5)) +
  xlab("Coefficient Estimate") +
  ylab("weights") +
  ggtitle("Coefficients and Confidence Intervals") +
  facet_wrap(~ arm, ncol = 2)

# Print the plot
print(plot)
```


## For the non-contextual case

We run a non-contextual experiment. 
```{r noncontextual}
results <- run_experiment(ys, floor_start, floor_decay, batch_sizes)

# inverse probability score 1[W_t=w]/e_t(w) of pulling arms, shape [A, K]
balwts <- calculate_balwts(results$ws, results$probs)

# plot the cumulative assignment graph for every arm and every batch size, x-axis is the number of observations, y-axis is the cumulative assignment
plot_cumulative_assignment(results, batch_sizes)


aipw_scores_learn <- aw_scores(
  ws = results$ws, 
  yobs = results$yobs, 
  K = ncol(results$ys),
  balwts = balwts)

```

```{r estimation_nc}
# get two-dimensional probabilities
# TODO explain what's going on with probabilities
# TODO what happens with contextual variances when there's 2d probs/
probs <- results$probs[,1,]
# Get estimates for policies

## Define counterfactual policies
### Control policy matrix policy0. This is a matrix with A rows and K columns, where the elements in the first column are all 1s and the elements in the remaining columns are all 0s.
policy0 <- matrix(0, nrow = A, ncol = K)
policy0[,1] <- 1

### Treatment policies list policy1. This is a list with K elements, where each list contains a matrix with A rows and K columns. Identifier of treatment x: the x th column of the matrix in the x th policy in the list is 1.
policy1 <- lapply(1:K, function(x) {
  pol_mat <- matrix(0, nrow = A, ncol = K)
  pol_mat[,x] <- 1
  pol_mat
}
) 

## estimating the value Q(w) of a single arm w. Here we estimate all the arms in policy1 in turn. 
out_full <- output_estimates(
  policy1 = policy1, 
  gammahat = aipw_scores_learn, 
  contextual_probs = probs)

# Get estimates for treatment effects of policies as contrast to control \delta(w_1, w_2) = E[Y_t(w_1) - Y_t(w_2)]. In Hadad, Vitor, et al (2021) there are two approaches.
## The first approach: use the difference in AIPW scores as the unbiased scoring rule for \delta (w_1, w_2)
### The following function implements the first approach by subtracting policy0, the control arm, from all the arms in policy1, except for the control arm itself.
out_full_te1 <- output_estimates(
  policy0 = policy0,
  policy1 = policy1[-1],  ## remove the control arm from policy1
  contrasts = 'combined',
  gammahat = aipw_scores_learn, 
  contextual_probs = probs)

## The second approach takes asymptotically normal inference about \delta(w_1, w_2): \delta ^ hat (w_1, w_2) = Q ^ hat (w_1) - Q ^ hat (w_2)
out_full_te2 <- output_estimates(
  policy0 = policy0,
  policy1 = policy1[-1],  ## remove the control arm from policy1
  contrasts = 'separate',
  gammahat = aipw_scores_learn, 
  contextual_probs = probs)


# Figure

# Combine the data frames into a single data frame
combinedMatrix <- do.call(rbind, out_full_te2)
combinedMatrix <- as.data.frame(combinedMatrix)

# Add an .id column to indicate the arm number
n_arms <- length(out_full_te2)
n_reps <- nrow(out_full_te2[[1]])
combinedMatrix$id <- rep(1:n_arms, each = n_reps)

# Add a column to indicate the arm name
n_arms <- n_arms+1
combinedMatrix$arm <- rep(paste0("arm", 2:n_arms), each = n_reps)

# Add a method column
values <- c("uniform", "non_contextual_minvar", "contextual_minvar", 
            "non_contextual_stablevar", "contextual_stablevar", "non_contextual_twopoint")
combinedMatrix$method <- rep(values, length.out = nrow(combinedMatrix))


# Create the plot
plot <- ggplot(combinedMatrix, aes(x = estimate, y = method)) +
  geom_point(aes(x = estimate), size = 2, position = position_dodge(width=0.5)) +
  geom_errorbar(aes(xmin = estimate - 1.96*std.error, xmax = estimate + 1.96*std.error), width = 0.05, 
                position = position_dodge(width=0.5)) +
  xlab("Coefficient Estimate") +
  ylab("weights") +
  ggtitle("Coefficients and Confidence Intervals") +
  facet_wrap(~ arm, ncol = 2)

# Print the plot
print(plot)
```
